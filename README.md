# Silent-Voice: Sign Language Translator
Real-time Indian Sign Language (ISL) gesture recognition using Edge AI on the Arduino Nicla Vision  
On-device inference Â· Wi-Fi video streaming Â· Optional display mode

---

## ðŸ“Œ Overview
**Silent-Voice** is a portable, real-time sign language translator powered entirely by **edge machine learning** running on the **Arduino Nicla Vision**.  
It recognizes **15 Indian Sign Language (ISL) gestures** using an **on-device object detection model** trained with **Edge Impulse**, and displays the gesture labels directly on:

- a **live Wi-Fi MJPEG video stream** accessible via a smartphone browser, or  
- an **optional display** connected to the Nicla Vision  

The entire inference pipeline runs **offline**, without any cloud or external computation.

---

## ðŸ“Œ Features
- âœ” Real-time ISL gesture recognition  
- âœ” Entirely offline â€” no cloud required  
- âœ” Model trained using **MobileNetV2 0.35**  
- âœ” Input size **96Ã—96**, **grayscale**, **INT8 optimized**  
- âœ” Wi-Fi MJPEG streaming using OpenMV  
- âœ” Optional display mode (TFT LCD / ST7789 / ILI9341)  
- âœ” 3,036-image custom dataset collected using Nicla Vision  
- âœ” Annotated with bounding boxes in Edge Impulse Labeling UI  
- âœ” Confusion matrix and validation metrics included  
- âœ” Works on any smartphone browser  

---

## ðŸ“Œ Dataset Summary
**Total images:** 3,036  
**Classes:** 15 ISL gestures  
**Images per class:** ~200  
**Collected using:** Arduino Nicla Vision (OpenMV)  
**Annotated in:** Edge Impulse (manual bounding boxes)  
**Participants:** 4 subjects (ensures diversity)

### Class List:
`agree, angry, bad, come, fine, go, happy, hello, how, hungry, me, please, sorry, thank, you`


---

## ðŸ“Œ Model Information
- **Model type:** MobileNetV2 0.35  
- **Impulse Input:** 96Ã—96  
- **Color:** Grayscale  
- **Quantization:** INT8  
- **Validation Accuracy:** 93.1%  
- **Test Accuracy:** 90.17%  
- **Precision:** 0.96  
- **Recall:** 0.90  
- **F1 Score:** 0.93  
- **Inference Time:** 691 ms  
- **Peak RAM Usage:** 137.8 KB  
- **Flash Usage:** 81.9 KB  

---

## ðŸ“Œ System Architecture
`Camera Frame â†’ Model Inference â†’ Gesture Bounding Box â†’ Gesture Label Overlay â†’ HTTP MJPEG Stream â†’ Phone Browser (real-time output)`

Optional: `Camera Frame â†’ Gesture Overlay â†’ External Display`


---

## ðŸ“Œ Project Structure

```
Silent-Voice/
â”‚
â”œâ”€â”€ README.md                    # Full documentation
â”œâ”€â”€ LICENSE                      # MIT recommended
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained.tflite
â”‚   â”œâ”€â”€ labels.txt
â”‚   â””â”€â”€ model_metadata.json
â”‚
â”œâ”€â”€ firmware/
â”‚   â”œâ”€â”€ silent_voice_openmv.py   # Main inference + streaming script
â”‚   â”œâ”€â”€ display_mode.py          # Optional display-connected script
â”‚   â””â”€â”€ wifi_credentials.txt     # Template
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ dataset_summary.md       # Class counts, methodology
â”‚   â””â”€â”€ samples/                 # (Optional few samples, dataset is private)
â”‚
â”œâ”€â”€ edge-impulse/
â”‚   â”œâ”€â”€ impulse_screenshot.png
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ validation_metrics.png
â”‚   â””â”€â”€ project_readme.md        # Edge Impulse dashboard README
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ system_flowchart.png
â”‚   â”œâ”€â”€ hardware_setup.png
â”‚   â””â”€â”€ methodology.pdf          # Optional
â”‚
â””â”€â”€ demo/
    â”œâ”€â”€ demo_video_link.txt
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ hand_detection_sample.png
    â”‚   â”œâ”€â”€ gesture_classification_sample.png
    â””â”€â”€ streaming_screenshot.png
```

---

## ðŸ“Œ Deployment (Nicla Vision + OpenMV)
### **1. Install OpenMV IDE**  
Download for Windows.

### **2. Flash Nicla Vision with latest OpenMV firmware**

### **3. Copy these files to Nicla Vision's storage:**
- `trained.tflite`  
- `labels.txt`  
- `silent_voice_openmv.py`  

### **4. Update Wi-Fi credentials**
Edit:
```python
SSID = "YourHotspotName"
KEY  = "YourPassword"

### **Run the script in OpenMV IDE**
### **6. Open the live video stream**

On your smartphone or laptop: http://<NICLA-IP>:8080

You will see:
-Live camera feed
-Gesture label drawn next to the detected sign

## ðŸ“Œ Results

- ðŸŽ¯ Stable real-time gesture recognition  
- ðŸŽ¯ Robust performance across different users & lighting conditions  
- ðŸŽ¯ Works instantly on any phone browser  
- ðŸŽ¯ Accurate bounding box + gesture label overlay  
- ðŸŽ¯ No app installation required  

---

## ðŸ“Œ Demo

ðŸ“º **Video:** _Add your YouTube link here_  
ðŸ“¸ **Screenshots:** Available in `/demo/images/`  

---

## ðŸ“Œ Edge Impulse Public Project

ðŸ”— **Link:** _Add your EI public project link here_  

---

## ðŸ“Œ Hardware Used

- Arduino Nicla Vision  
- USB-C Cable  
- Smartphone Hotspot  
- Optional: TFT LCD Display (ST7789 / ILI9341)  

---

## ðŸ“Œ License

MIT License  

---

## ðŸ“Œ Acknowledgements

- Edge Impulse  
- Arduino Nicla Vision  
- OpenMV  
- Dataset Volunteers  
